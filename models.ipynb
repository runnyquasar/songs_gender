{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_yye2GVkhae"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s37Mf6wHM2qs"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2Hv20j0hbhW"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4TMtzXML6Mx"
      },
      "source": [
        "df = pd.read_csv('/content/result.csv', encoding='utf-8')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6dFYkXj8bXd"
      },
      "source": [
        "big_df = pd.read_csv('/content/more_data.csv', encoding='utf-8')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIRw1AQ78gJ4"
      },
      "source": [
        "bigger_df = pd.read_csv('/content/bigger_data.csv', sep=';', encoding='utf-8')"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "728oAwBkM8yl"
      },
      "source": [
        "df = df.drop(columns='Unnamed: 0')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug9PVSzs_h9t"
      },
      "source": [
        "big_df = big_df.drop(columns='Unnamed: 0')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbwI_tDpA_XA"
      },
      "source": [
        "bigger_df = bigger_df.drop(columns='Column1')"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvC6NmQKNix5"
      },
      "source": [
        "bigger_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_HuHeQLO9n8"
      },
      "source": [
        "sentences = bigger_df.tokens.values\n",
        "labels = bigger_df.gender.values"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnoDzBjYO2-S",
        "outputId": "85f902b9-9efc-4428-d697-1056424f8f88"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)\n",
        "print('Original: ', sentences[0])\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  MASK басса доброго крути MASK сегодня красиво сатива MASK вечер MASK мимо лечим лечим мокрого сухого принимай слабо дом боба засовы MASK всё MASK честь битов MASK облака энди мияги стиль пластик пламя MASK приведи бог музы MASK время песок ман MASK пора солнце MASK MASK тела туса MASK теле MASK видом ночью MASK MASK акелла MASK сегодня сегодня сегодня мои раста MASK панчи час чары чики нунчаки MASK санчо молча ночь MASK части тела погромче дыма MASK антанта ламба вдаль ламбо бланта типа ван дамм MASK битам инфаркта инферно MASK густо MASK MASK MASK момент MASK MASK MASK фейрверк тук тук MASK звук мои пацаны MASK круг раггамафин фоном фараонам братьям салам всем джа MASK баронам сегодня сегодня сегодня сегодня сегодня сегодня\n",
            "Tokenized:  ['mask', 'бас', '##са', 'добро', '##го', 'к', '##ру', '##ти', 'mask', 'сегодня', 'к', '##рас', '##ив', '##о', 'са', '##тив', '##а', 'mask', 'вече', '##р', 'mask', 'ми', '##мо', 'ле', '##чим', 'ле', '##чим', 'мо', '##к', '##ро', '##го', 'су', '##хо', '##го', 'при', '##нима', '##и', 'слабо', 'дом', 'боб', '##а', 'за', '##сов', '##ы', 'mask', 'все', 'mask', 'честь', 'бит', '##ов', 'mask', 'обл', '##ака', 'э', '##нди', 'ми', '##яг', '##и', 'стиль', 'п', '##лас', '##тик', 'п', '##лам', '##я', 'mask', 'при', '##ве', '##ди', 'бог', 'му', '##зы', 'mask', 'время', 'п', '##ес', '##ок', 'ма', '##н', 'mask', 'пор', '##а', 'сол', '##нце', 'mask', 'mask', 'тела', 'ту', '##са', 'mask', 'теле', 'mask', 'вид', '##ом', 'ночь', '##ю', 'mask', 'mask', 'ак', '##ел', '##ла', 'mask', 'сегодня', 'сегодня', 'сегодня', 'мои', 'ра', '##ста', 'mask', 'па', '##нчи', 'час', 'ч', '##ары', 'чи', '##ки', 'н', '##ун', '##чак', '##и', 'mask', 'сан', '##чо', 'мо', '##л', '##ча', 'ночь', 'mask', 'части', 'тела', 'по', '##гр', '##ом', '##че', 'д', '##ым', '##а', 'mask', 'ан', '##тан', '##та', 'ла', '##мба', 'в', '##дал', '##ь', 'ла', '##м', '##бо', 'б', '##лант', '##а', 'типа', 'ван', 'да', '##м', '##м', 'mask', 'бит', '##ам', 'ин', '##фа', '##рк', '##та', 'ин', '##фер', '##но', 'mask', 'г', '##ус', '##то', 'mask', 'mask', 'mask', 'момент', 'mask', 'mask', 'mask', 'ф', '##еи', '##рв', '##ер', '##к', 'тук', 'тук', 'mask', 'звук', 'мои', 'па', '##цан', '##ы', 'mask', 'круг', 'ра', '##г', '##гам', '##а', '##фи', '##н', 'фон', '##ом', 'ф', '##ара', '##она', '##м', 'братья', '##м', 'са', '##лам', 'всем', 'дж', '##а', 'mask', 'барон', '##ам', 'сегодня', 'сегодня', 'сегодня', 'сегодня', 'сегодня', 'сегодня']\n",
            "Token IDs:  [44054, 20004, 12109, 48566, 11141, 316, 14148, 11095, 44054, 49310, 316, 52930, 17281, 10353, 10957, 63409, 10185, 44054, 39270, 10584, 44054, 28830, 18095, 43247, 49902, 43247, 49902, 74584, 10506, 15491, 11141, 10626, 37489, 11141, 10842, 92178, 10178, 82109, 17928, 94691, 10185, 10242, 27982, 10328, 44054, 12506, 44054, 19959, 91268, 10475, 44054, 56361, 35532, 335, 48898, 28830, 71930, 10178, 54525, 321, 48922, 27900, 321, 72427, 10401, 44054, 10842, 14480, 12377, 43663, 12653, 21376, 44054, 11841, 321, 18623, 12347, 68101, 10300, 44054, 39800, 10185, 23794, 81520, 44054, 44054, 32415, 26036, 12109, 44054, 60938, 44054, 15642, 10392, 50805, 10640, 44054, 44054, 43645, 14549, 10726, 44054, 49310, 49310, 49310, 55566, 90137, 15294, 44054, 12682, 67362, 12795, 329, 29817, 17761, 10736, 319, 17256, 62032, 10178, 44054, 15436, 32562, 74584, 10571, 13291, 50805, 44054, 14864, 32415, 10291, 74709, 10392, 13461, 311, 13716, 10185, 44054, 21530, 15072, 10376, 16820, 88723, 309, 39029, 10979, 16820, 10260, 29844, 308, 95803, 10185, 22161, 27960, 10448, 10260, 10260, 44054, 91268, 13192, 18745, 17338, 53464, 10376, 18745, 43654, 10758, 44054, 310, 19901, 10908, 44054, 44054, 44054, 20887, 44054, 44054, 44054, 326, 11850, 51947, 12576, 10506, 77406, 77406, 44054, 71775, 55566, 12682, 89803, 10328, 44054, 65547, 90137, 11090, 83315, 10185, 22936, 10300, 15686, 10392, 326, 18641, 15204, 10260, 84997, 10260, 10957, 72427, 39026, 42817, 10185, 44054, 72816, 13192, 49310, 49310, 49310, 49310, 49310, 49310]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsx4yAf9PGi4",
        "outputId": "04a597af-c07e-4877-de9f-72364e18cd98"
      },
      "source": [
        "input_ids = []\n",
        "for sent in sentences:\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        str(sent),                      \n",
        "                        add_special_tokens = True)\n",
        "    input_ids.append(encoded_sent)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (606 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQOF93HBPO_f"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "MAX_LEN = 256\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN , truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QmZCWV1PUE-"
      },
      "source": [
        "attention_masks = []\n",
        "for sent in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq1o_ZDfPcjq"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,test_size=0.1)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2Al1uN1Py4w"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-G1k3rlPtOq"
      },
      "source": [
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu9y8wKFQthn",
        "outputId": "a72ce5c0-f8a4-4c3e-bbe6-666c8fc33138"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-uncased\",\n",
        "    num_labels = 2,   \n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KNcrJw_Q-cB"
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr = 2e-5)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upvs-iP_RJ-e",
        "outputId": "eb45a265-efc8-4620-a61d-6566ae7653f2"
      },
      "source": [
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPde0TghRTFm",
        "outputId": "1dd92d08-a9b4-419c-937d-063f30009964"
      },
      "source": [
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (105879, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM0q8Vq7RaZB"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 3\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTxI0yr8RgXR"
      },
      "source": [
        "import numpy as np\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfLhSUc4RjZ4"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSKJVJltgvcb"
      },
      "source": [
        "acc_list = []\n",
        "ep_list = []"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71hjjjbqRpSK",
        "outputId": "b1a35e49-e427-4ee8-abc4-95e2a2a2bb89"
      },
      "source": [
        "import random\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "loss_values = []\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        model.zero_grad()        \n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        loss = outputs[0]\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "    \n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        logits = outputs[0]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        nb_eval_steps += 1\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "    acc_list.append(eval_accuracy/nb_eval_steps)\n",
        "    ep_list.append(epoch_i + 1)\n",
        "\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of     85.    Elapsed: 0:00:58.\n",
            "  Batch    80  of     85.    Elapsed: 0:01:55.\n",
            "\n",
            "  Average training loss: 0.66\n",
            "  Training epcoh took: 0:02:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of     85.    Elapsed: 0:00:57.\n",
            "  Batch    80  of     85.    Elapsed: 0:01:54.\n",
            "\n",
            "  Average training loss: 0.52\n",
            "  Training epcoh took: 0:02:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.79\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of     85.    Elapsed: 0:00:57.\n",
            "  Batch    80  of     85.    Elapsed: 0:01:54.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epcoh took: 0:02:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0WmfctHWHdJ"
      },
      "source": [
        "torch.cuda.empty_cache() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlRuwH-Jm5Mx",
        "outputId": "29619129-5c54-4f35-b011-9444024b16ee"
      },
      "source": [
        "acc_list"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.546875,\n",
              " 0.5390625,\n",
              " 0.578125,\n",
              " 0.5803571428571429,\n",
              " 0.6651785714285714,\n",
              " 0.6607142857142857,\n",
              " 0.7,\n",
              " 0.7947916666666667,\n",
              " 0.825]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMsaR-tJDDIU"
      },
      "source": [
        "data_size = [1000, 1000, 1000, 2000, 2000, 2000, 3000, 3000, 3000]"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVOt5NIlC4jj"
      },
      "source": [
        "stats = pd.DataFrame(columns=['epoch', 'acc', 'data_size'])"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf8BxqoiDJy0"
      },
      "source": [
        "stats['data_size'] = data_size\n",
        "stats['epoch'] = ep_list\n",
        "stats['acc'] = acc_list"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "YKQqdNVGDVI9",
        "outputId": "c775ea1a-2c5a-4ff6-ee72-b83fb37a89f5"
      },
      "source": [
        "stats"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch</th>\n",
              "      <th>acc</th>\n",
              "      <th>data_size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.546875</td>\n",
              "      <td>1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.539062</td>\n",
              "      <td>1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.578125</td>\n",
              "      <td>1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0.580357</td>\n",
              "      <td>2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>0.665179</td>\n",
              "      <td>2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3</td>\n",
              "      <td>0.660714</td>\n",
              "      <td>2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>3000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2</td>\n",
              "      <td>0.794792</td>\n",
              "      <td>3000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>0.825000</td>\n",
              "      <td>3000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   epoch       acc  data_size\n",
              "0      1  0.546875       1000\n",
              "1      2  0.539062       1000\n",
              "2      3  0.578125       1000\n",
              "3      1  0.580357       2000\n",
              "4      2  0.665179       2000\n",
              "5      3  0.660714       2000\n",
              "6      1  0.700000       3000\n",
              "7      2  0.794792       3000\n",
              "8      3  0.825000       3000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "QXM8_yS5Drx2",
        "outputId": "7c69a978-8881-452e-8379-0e93107aac98"
      },
      "source": [
        "stats.plot(y='acc', x='data_size')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f8db1440fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEHCAYAAACgHI2PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAcJElEQVR4nO3de5BedZ3n8fcnnRshnXtCN0kgDSbpcEuABi+MLisLBBxJyvESnK3Fy8JcRHd1tSqULjo4bjFjWeNSmxqNVkqcGg14wcrMRlkYRXaVaBpFMVdCotIxlzYJSYfcu7/7x/l153TTl6fpp/vpPvm8qp7KeX7nd875PqdPPn36/M7zPIoIzMysuEZVugAzMxtcDnozs4Jz0JuZFZyD3sys4Bz0ZmYFN7rSBXQ1Y8aMmDdvXqXLMDMbUZ599tk/RsTM7uYNu6CfN28ejY2NlS7DzGxEkfS7nub50o2ZWcE56M3MCs5Bb2ZWcMPuGn13Tp8+TVNTEydOnKh0KWUzfvx45syZw5gxYypdipkV3IgI+qamJqqrq5k3bx6SKl3OgEUEBw4coKmpibq6ukqXY2YFNyIu3Zw4cYLp06cXIuQBJDF9+vRC/YViZsPXiAh6oDAh365or8fMhq8RE/RmZkX25OZ9fP/5PYOy7hFxjd7MrOi+vuF3HDl+mtuurC37un1Gb2ZWcA76fli+fDnXXnstl19+OatXrwbgBz/4Addccw2LFy/mpptuAuDo0aO8//3v58orr+Sqq67iO9/5TiXLNrNz3Ii7dPM3/7KJzX84UtZ1XnbhJD799sv77LdmzRqmTZvG8ePHue6661i2bBl33303Tz/9NHV1dRw8eBCAz372s0yePJnnn38egEOHDpW1XjOz/hhxQV9JDz30EI899hgAL730EqtXr+Ytb3lLx73w06ZNA+DJJ59k7dq1HctNnTp16Is1M0tGXNCXcuY9GJ566imefPJJnnnmGSZMmMCNN97IkiVL2Lp1a0XqMTMrla/Rl+jw4cNMnTqVCRMmsHXrVjZs2MCJEyd4+umn2bVrF0DHpZubb76ZVatWdSzrSzdmVkkO+hItXbqUM2fOsGjRIlauXMkb3vAGZs6cyerVq3nHO97B4sWLec973gPApz71KQ4dOsQVV1zB4sWL+dGPflTh6s3sXDbiLt1Uyrhx4/j+97/f7bzbbrut0/OJEyfy8MMPD0VZZmZ98hm9mVnBOejNzApuxAR9RFS6hLIq2usxs+FrRAT9+PHjOXDgQGHCsf3z6MePH1/pUszsHDAiBmPnzJlDU1MTzc3NlS6lbNq/YcrMbLCVFPSSlgL/E6gCvhoRD3aZfxHwMDAl9VkZEevTvPuADwKtwEci4vH+FjlmzBh/E5OZ2WvUZ9BLqgJWATcDTcBGSesiYnOu26eARyPiHyVdBqwH5qXpFcDlwIXAk5IWRERruV+ImZl1r5Rr9NcDOyJiZ0ScAtYCy7r0CWBSmp4M/CFNLwPWRsTJiNgF7EjrMzOzIVJK0M8GXso9b0pteZ8B/qOkJrKz+Q/3Y1kzMxtE5brr5k7gaxExB7gd+CdJJa9b0j2SGiU1FmnA1cxsOCgljHcDc3PP56S2vA8CjwJExDPAeGBGicsSEasjoiEiGmbOnFl69WZm1qdSgn4jMF9SnaSxZIOr67r0+T1wE4CkRWRB35z6rZA0TlIdMB/4ebmKNzOzvvV5101EnJF0L/A42a2TayJik6QHgMaIWAf8N+Arkj5KNjD7vsje3bRJ0qPAZuAM8CHfcWNmNrRKuo8+3RO/vkvb/bnpzcANPSz7OeBzA6jRzMwGYER8BIKZmb12Dnozs4Jz0JuZFZyD3sys4Bz0ZmYF56A3Mys4B72ZWcE56M3MCs5Bb2ZWcA56M7OCc9CbmRWcg97MrOAc9GZmBeegNzMrOAe9mVnBOejNzArOQW9mVnAOejOzgnPQm5kVnIPezKzgHPRmZgXnoDczKzgHvZlZwTnozcwKzkFvZlZwDnozs4IrKeglLZW0TdIOSSu7mf8Pkp5Lj+2SXs7Na83NW1fO4s3MrG+j++ogqQpYBdwMNAEbJa2LiM3tfSLio7n+Hwauzq3ieEQsKV/JZmbWH6Wc0V8P7IiInRFxClgLLOul/53AN8tRnJmZDVwpQT8beCn3vCm1vYqki4E64Ie55vGSGiVtkLS8h+XuSX0am5ubSyzdzMxKUe7B2BXAtyOiNdd2cUQ0AO8Fvijp0q4LRcTqiGiIiIaZM2eWuSQzs3NbKUG/G5ibez4ntXVnBV0u20TE7vTvTuApOl+/NzOzQVZK0G8E5kuqkzSWLMxfdfeMpHpgKvBMrm2qpHFpegZwA7C567JmZjZ4+rzrJiLOSLoXeByoAtZExCZJDwCNEdEe+iuAtRERucUXAV+W1Eb2S+XB/N06ZmY2+PoMeoCIWA+s79J2f5fnn+lmuZ8CVw6gPjMzGyC/M9bMrOAc9GZmBeegNzMrOAe9mVnBOejNzArOQW9mVnAOejOzgnPQm5kVnIPezKzgHPRmZgXnoDczKzgHvZlZwTnozcwKzkFvZlZwDnozs4Jz0JuZFZyD3sys4Bz0ZmYF56A3Mys4B72ZWcE56M3MCs5Bb2ZWcA56M7OCc9CbmRWcg97MrOBKCnpJSyVtk7RD0spu5v+DpOfSY7ukl3Pz7pL0QnrcVc7izcysb6P76iCpClgF3Aw0ARslrYuIze19IuKjuf4fBq5O09OATwMNQADPpmUPlfVVmJlZj0o5o78e2BEROyPiFLAWWNZL/zuBb6bpW4EnIuJgCvcngKUDKdjMzPqnzzN6YDbwUu55E/D67jpKuhioA37Yy7Kz+1+mmVlxHD/Vygv7W9i6p4Wte1vYuvcIv/j9IS6/cPKgbK+UoO+PFcC3I6K1PwtJuge4B+Ciiy4qc0lmZpXR1hY0HTrOlr1H2JYCfeueFn574BXaIutz3pgqFtRUs3zJbO5YcuGg1FFK0O8G5uaez0lt3VkBfKjLsjd2WfaprgtFxGpgNUBDQ0OUUJOZ2bBy+PjpjjDfsif7d/veFl45lZ33SnDxtAnU10zijiUXUl9TTX3NJC6aNoFRozSotZUS9BuB+ZLqyIJ7BfDerp0k1QNTgWdyzY8D/0PS1PT8FuC+AVVsZlZBp1vb2PXHV9iy5whb97Zk4b7nCH84fKKjz5QJY6ivqeZdDXOzQK+dxIILJjJhbLkvopSmz61GxBlJ95KFdhWwJiI2SXoAaIyIdanrCmBtRERu2YOSPkv2ywLggYg4WN6XYGZWfhFBc8tJtqQg37a3hS17W3hx/1FOtbYBMKZKXDpzItfXTaO+dlLHWfoFk8YhDe5Zen8ol8vDQkNDQzQ2Nla6DDM7hxw/1cr2fS0pzI+kQdIjHDp2uqNP7eTxLExBvqi2moU11VwyYyJjRw+P951KejYiGrqbV5m/I8zMKqCtLXjp0LHsTpcU5tv2trDrwCu0n/NOGFvFgguqWXpFDfU1k1K4VzNlwtjKFj8ADnozK6TDx05nd7m03+2Srqcfyw2Ozpt+PvU11WlwNDtTnzt18AdHh5qD3sxGtNOtbexsfuVsqKdB0j1dBkcX1Uzi3Q1zWVSbXX6ZX8HB0aF2brxKMxvxIoL9LSfZsqf9nvQWtuw5wovNRzndml13aR8cfcMl06mvya6jL6qdxKzq4TU4OtQc9GY27LQPjubvSd+2t+VVg6P1NdX8+/pZHXe7XDLzfMZUDY/B0eHEQW9mFdM+OJoP8617s3eO5gdHF9ZUs/SK2hToWahPnjCmssWPIA56MxsSLx871XENfdu+FrbsaWH7vs6Do3VpcHT5ktnU11azqGYSc6aeV7jB0aHmoDezsjp1po2dfzza6QO7tu5pYe+Rs4OjUyeMob5mEu+5bi6L0i2MCy6o5ryxVRWsvLgc9Gb2mkQE+46cfNXdLl0HR183q5o3XTqd+tpqFtZMYlFNNTPP8cHRoeagN7M+HTt1hu37jnaEeXu4v5wbHL1w8njqayd1DI4uqp1E3QwPjg4HDnoz69DWFvz+4LGOu13aP43xdwePdQyOnp8GR2+7orbjnvSFF1R7cHQYc9CbnaMOvXIqvVs0OzvfsreF7XtbOH46Gxwdld45etmFk3jHNXM67nbx4OjI46A3K7hTZ9p4sflopw/s2ra38+DotPPHUl9TzZ3XX5Q+Vrea+bM8OFoUDnqzgogI9h458aoP7Nqx/yhn0tcZja0axetmTeRNr5vecYZeX1vNzIkeHC0yB73ZCPTKyTPpnaMtuQHSFg4fPzs4OnvKeSysqeat9bOor83udpnnwdFzkoPebBhrbR8c7XK3y++7GRx921W1LKrJbmFcWFPN5PM8OGoZB73ZMHHolVNnv0S6/TtH9x3tPDg643yuuHAy77xmTscHds2e4sFR652D3myInTzTyov7X2HbvmxgdEu682XfkZMdfaadP5ZFtWlwNH0UwPwLJjJ+jAdHrf8c9GYlmrfyf/P6umk88hdvLKl/x+DonpZOZ+ovNnceHJ1/wURueN0MFqWB0YU1Hhy18nLQm/XDz3Z1/932r5w8w7Z9LenWxSMdXyh95MSZjj6zp5xHfU01/+GyWdndLh4ctSHioDfrp50d96SfvePl9wePdcyfOG40C2uqefviC9M96ZNYcIEHR61yHPRm/fTWL/wYyAZH62acz5VzJvOua+dQX5udpc+Zep4vu9iw4qA366fPv/Oqju8c9eCojQQOerMSzaoex02LZvGuhrmVLsWsXzwKZGZWcA56M7OCKynoJS2VtE3SDkkre+jzbkmbJW2S9I1ce6uk59JjXbkKNzOz0vR5jV5SFbAKuBloAjZKWhcRm3N95gP3ATdExCFJs3KrOB4RS8pct5mZlaiUM/rrgR0RsTMiTgFrgWVd+twNrIqIQwARsb+8ZZqZ2WtVStDPBl7KPW9KbXkLgAWSfiJpg6SluXnjJTWm9uXdbUDSPalPY3Nzc79egJmZ9a5ct1eOBuYDNwJzgKclXRkRLwMXR8RuSZcAP5T0fES8mF84IlYDqwEaGhqiTDWZmRmlndHvBvI3Ds9JbXlNwLqIOB0Ru4DtZMFPROxO/+4EngKuHmDNZmbWD6UE/UZgvqQ6SWOBFUDXu2e+R3Y2j6QZZJdydkqaKmlcrv0GYDNmZjZk+rx0ExFnJN0LPA5UAWsiYpOkB4DGiFiX5t0iaTPQCnwiIg5IehPwZUltZL9UHszfrWNmZoOvpGv0EbEeWN+l7f7cdAAfS498n58CVw68TDMze638zlgzs4Jz0JuZFZyD3sys4Bz0ZmYF56A3Mys4B72ZWcE56M3MCs5Bb2ZWcA56M7OCc9CbmRWcg97MrOAc9GZmBeegNzMrOAe9mVnBOejNzArOQW9mVnAOejOzgnPQm5kVnIPezKzgHPRmZgXnoDczKzgHvZlZwTnozcwKzkFvZlZwDnozs4Jz0JuZFVxJQS9pqaRtknZIWtlDn3dL2ixpk6Rv5NrvkvRCetxVrsLNzKw0o/vqIKkKWAXcDDQBGyWti4jNuT7zgfuAGyLikKRZqX0a8GmgAQjg2bTsofK/FDMz604pZ/TXAzsiYmdEnALWAsu69LkbWNUe4BGxP7XfCjwREQfTvCeApeUp3czMSlFK0M8GXso9b0pteQuABZJ+ImmDpKX9WBZJ90hqlNTY3NxcevVmZtancg3GjgbmAzcCdwJfkTSl1IUjYnVENEREw8yZM8tUkpmZQWlBvxuYm3s+J7XlNQHrIuJ0ROwCtpMFfynLmpnZICol6DcC8yXVSRoLrADWdenzPbKzeSTNILuUsxN4HLhF0lRJU4FbUpuZmQ2RPu+6iYgzku4lC+gqYE1EbJL0ANAYEes4G+ibgVbgExFxAEDSZ8l+WQA8EBEHB+OFmJlZ9/oMeoCIWA+s79J2f246gI+lR9dl1wBrBlammZm9Vn5nrJlZwTnozcwKzkFvZlZwDnozs4Jz0JuZFZyD3sys4Eq6vdJsOIoIWtuCtoC2jumgrS09j6AtzT873XmZ9ucR0NqWLZOtN81PbW0BJ8+0Vfolm70mDvp+iugmWFJItAdPFhZ0Cp7WiI7gaMvP609YtW+jvYa26LSeTuttC1qDXmrqvJ62fLjla+ro11No0lFTPjC7rSnfv+SaOu+bSNtvbYuK/Pyrx4+pyHbNBqIwQd/aFnz8W79i96HjfYZqPix6DKb2oInOgReVyZdBUTVKjBKMkhglnX0+SlRJSKJqFLnpzvNHSYxKbdm8s9OSGD1qVJrOz8/WeXZZUZXWmU2LUWl+fpmObbRvL1dTxzZyNeXX2WmbUqeaen9tneuoGiUW1VZX+sdm1m+FCfqXj53isV/uZt70CVw45byOsKkSuf/8Z//zdg6Ss+FUcrDlg+pV6+waXF22kZaRSg227vp1Waa7uiSUXk93YWhm54bCBH27D/xJHf/pjfMqXYaZ2bDhu27MzArOQW9mVnAOejOzgnPQm5kVnIPezKzgHPRmZgXnoDczKzgHvZlZwTnozcwKzkFvZlZwDnozs4Jz0JuZFZyD3sys4Bz0ZmYFV1LQS1oqaZukHZJWdjP/fZKaJT2XHv85N681176unMWbmVnf+vw8eklVwCrgZqAJ2ChpXURs7tL1kYi4t5tVHI+IJQMv1czMXotSzuivB3ZExM6IOAWsBZYNbllmZlYupQT9bOCl3POm1NbVn0n6taRvS5qbax8vqVHSBknLu9uApHtSn8bm5ubSqzczsz6VazD2X4B5EXEV8ATwcG7exRHRALwX+KKkS7suHBGrI6IhIhpmzpz5mgpoTd/afeT46de0vJlZUZUS9LuB/Bn6nNTWISIORMTJ9PSrwLW5ebvTvzuBp4CrB1Bvjw4fywL+e8/9YTBWb2Y2YpUS9BuB+ZLqJI0FVgCd7p6RVJt7egewJbVPlTQuTc8AbgC6DuKamdkg6vOum4g4I+le4HGgClgTEZskPQA0RsQ64COS7gDOAAeB96XFFwFfltRG9kvlwW7u1jEzs0HUZ9ADRMR6YH2Xtvtz0/cB93Wz3E+BKwdYo5mZDYDfGWtmVnAOejOzgitM0NfNOB+AT9y6sMKVmJkNLyVdox8JRleN4rcPvq3SZZiZDTuFOaM3M7PuOejNzArOQW9mVnAOejOzgnPQm5kVnIPezKzgHPRmZgXnoDczKzhF+sKO4UJSM/C7AaxiBvDHMpVTTq6rf1xX/7iu/iliXRdHRLff3DTsgn6gJDWmb7QaVlxX/7iu/nFd/XOu1eVLN2ZmBeegNzMruCIG/epKF9AD19U/rqt/XFf/nFN1Fe4avZmZdVbEM3ozM8tx0JuZFdywD3pJayTtl/SbXNs0SU9IeiH9OzW1S9JDknZI+rWka3LL3JX6vyDprkGq6/OStqZtPyZpSmqfJ+m4pOfS40u5Za6V9Hyq+SFJGoS6PiNpd277t+fm3Ze2vU3Srbn2palth6SVA6mpl7oeydX0W0nPpfah3F9zJf1I0mZJmyT9l9Re0WOsl7oqeoz1UldFj7Fe6qroMSZpvKSfS/pVqutvUnudpJ+lbTwiaWxqH5ee70jz5+XW1e1+LElEDOsH8BbgGuA3uba/B1am6ZXA36Xp24HvAwLeAPwstU8DdqZ/p6bpqYNQ1y3A6DT9d7m65uX7dVnPz1OtSrXfNgh1fQb4eDd9LwN+BYwD6oAXgar0eBG4BBib+lxW7rq6zP8CcH8F9lctcE2arga2p/1S0WOsl7oqeoz1UldFj7Ge6qr0MZbWMTFNjwF+ltb9KLAitX8J+Ks0/dfAl9L0CuCR3vZjqXUM+zP6iHgaONileRnwcJp+GFiea/96ZDYAUyTVArcCT0TEwYg4BDwBLC13XRHxfyLiTHq6AZjT2zpSbZMiYkNkP82v515L2erqxTJgbUScjIhdwA7g+vTYERE7I+IUsDb1HZS60hnTu4Fv9raOQdpfeyLiF2m6BdgCzKbCx1hPdVX6GOtlf/VkSI6xvuqq1DGWjpOj6emY9AjgrcC3U3vX46v9uPs2cFOqvaf9WJJhH/Q9uCAi9qTpvcAFaXo28FKuX1Nq66l9MH2A7GygXZ2kX0r6saQ3p7bZqZahqOve9Of+mvbLEAyf/fVmYF9EvJBrG/L9lf5MvprsrGvYHGNd6sqr6DHWTV3D4hjrYX9V7BiTVJUuGe0nOwF4EXg59ws7v42O/ZLmHwamM8D9NVKDvkP6rTus7hGV9EngDPDPqWkPcFFEXA18DPiGpElDWNI/ApcCS1ItXxjCbZfiTjqfaQ35/pI0EfgO8F8j4kh+XiWPsZ7qqvQx1k1dw+IY6+XnWLFjLCJaI2IJ2V9f1wP1g7Gd3ozUoN+X/sRq/1Nrf2rfDczN9ZuT2npqLztJ7wP+FPjzFBCkP7cOpOlnyX6jL0g15P/0HpS6ImJfOtjagK9w9k++4bC/RgPvAB7J1Tuk+0vSGLJw+OeI+G5qrvgx1kNdFT/GuqtrOBxjveyvih9jaTsvAz8C3kh2yW90N9vo2C9p/mTgAAPdX6VezK/kgy4DJ8Dn6TxQ9vdp+m10Hij7eWqfBuwiGySbmqanDUJdS4HNwMwu/WaSBk7IBp92t2+fVw/83D4IddXmpj9Kdq0P4HI6D/DsJBskG52m6zg7UHZ5uevK7bMfV2p/pfV8Hfhil/aKHmO91FXRY6yXuip6jPVUV6WPsbSdKWn6POD/kv2S/hadB2P/Ok1/iM6DsY/2th9LrmMg/0mG4kH259Ye4DTZdakPkl2z+jfgBeDJ3A9IwCqy387PAw259XyAbABjB/D+QaprB9l1tOfSo/0H9mfAptT2C+DtufU0AL9JNf8v0ruVy1zXP6X98WtgXZf/lJ9M295G7u4CsrtLtqd5nxyM/ZXavwb8ZZe+Q7m//oTsssyvcz+32yt9jPVSV0WPsV7qqugx1lNdlT7GgKuAX6a6fsPZu34uIfuFsoMs9Mel9vHp+Y40/5K+9mMpD38EgplZwY3Ua/RmZlYiB72ZWcE56M3MCs5Bb2ZWcA56M7OCc9CbmRWcg94KL32E7sd7mb9c0mVl3F6DpIfKtT6zgXLQm2WfHFi2oI+Ixoj4SLnWZzZQDnorJEmflLRd0v8DFqa2uyVtTF8C8R1JEyS9CbgD+Hz6AopLu+vXy3beJek3qe/Tqe1GSf+aptfr7JdbHFb25SRVyr5AZGP6tMe/GIJdYucwB70VjqRryT4nZAnZ2+yvS7O+GxHXRcRiss8r/2BE/JTsLfufiIglEfFid/162dz9wK2p7x1dZ0bE7ZF9cuEHgd8B30vThyPiulTb3ZLqBv7Kzbo3uu8uZiPOm4HHIuIYgKR1qf0KSX8LTAEmAo/3sHyp/QB+AnxN0qPAd7vrIGkG2WfBvDsiDku6BbhK0jtTl8nAfLIPQjMrOwe9nUu+BiyPiF+lj/q9cYD9iIi/lPR6sk+1fDb9NdFBUhXZtyc9EBHt35cr4MMR0dsvELOy8aUbK6KngeWSzpNUDbw9tVcDe9Lnlv95rn9Lmkcf/V5F0qUR8bOIuB9opvNnhgM8CPw6Itbm2h4H/iqtH0kLJJ3fv5doVjqf0VvhRMQvJD1C9vnd+4GNadZ/J/t6ueb0b3u4rwW+IukjwDt76dedz0uaT3aW/m9pm/8uN//jwKb0VXKQXdP/Ktln8/8ifR9oMwP87luz3vhjis3MCs6XbszMCs6XbsxKkL6M+11dmr8VEZ+rRD1m/eFLN2ZmBedLN2ZmBeegNzMrOAe9mVnBOejNzAru/wNZEf0PqvEzfgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}